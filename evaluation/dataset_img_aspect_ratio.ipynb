{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f503d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40d8dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 2975/2975 [08:22<00:00,  5.92it/s]\n",
      "Processing images: 100%|██████████| 500/500 [01:24<00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts — train/val/all: 2975 500 3475\n",
      "Width — min/median/max: 2048 2048 2048\n",
      "Height — min/median/max: 1024 1024 1024\n",
      "Aspect ratio (W/H) — min/median/max: 2.0 2.0 2.0\n",
      "Top resolutions (W,H,count):\n",
      "(2048, 1024, 3475)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "project_root = str(Path().absolute().parent)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Config selections\n",
    "DATASET = \"cityscapes\"  # hydra dataset_evaluation name\n",
    "MODEL = \"jafar\"\n",
    "TASK = \"seg\"\n",
    "\n",
    "# Initialize Hydra configuration\n",
    "if not GlobalHydra.instance().is_initialized():\n",
    "    initialize(config_path=\"../config\", version_base=None)\n",
    "cfg = compose(\n",
    "    config_name=\"eval\",\n",
    "    overrides=[\n",
    "        f\"dataset_evaluation={DATASET}\",\n",
    "        f\"eval.task={TASK}\",\n",
    "        f\"model={MODEL}\",\n",
    "        \"backbone.name='vit_small_patch16_dinov3.lvd1689m'\",\n",
    "        f\"project_root={project_root}\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Instantiate datasets WITHOUT transforms to read original sizes\n",
    "cfg.dataset_evaluation.split = \"train\"\n",
    "train_ds = instantiate(cfg.dataset_evaluation, transform=None, target_transform=None)\n",
    "\n",
    "cfg.dataset_evaluation.split = \"val\"\n",
    "val_ds = instantiate(cfg.dataset_evaluation, transform=None, target_transform=None)\n",
    "\n",
    "# Helper to compute sizes/ratios; supports datasets without explicit image_files by indexing\n",
    "\n",
    "def collect_sizes(dataset):\n",
    "    widths, heights, ratios = [], [], []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    has_files = hasattr(dataset, \"image_files\") and isinstance(getattr(dataset, \"image_files\"), (list, tuple))\n",
    "    if has_files:\n",
    "        iterable = dataset.image_files\n",
    "        def open_from_item(item):\n",
    "            return Image.open(item)\n",
    "    else:\n",
    "        iterable = range(len(dataset))\n",
    "        def open_from_item(item):\n",
    "            sample = dataset[item]\n",
    "            # Prefer path if present; otherwise infer from PIL image size before transforms\n",
    "            if isinstance(sample.get(\"image\"), Image.Image):\n",
    "                return sample[\"image\"]\n",
    "            # If tensor, its size is post-transform. Try to load raw file if available via common attrs\n",
    "            if hasattr(dataset, \"cityscapes_dataset\"):\n",
    "                # torchvision Cityscapes stores paths in images folder mirroring indices\n",
    "                img, _ = dataset.cityscapes_dataset[item]\n",
    "                return img\n",
    "            raise RuntimeError(\"Cannot access original image for this dataset\")\n",
    "\n",
    "    for item in tqdm(iterable, desc=\"Processing images\"):\n",
    "        with open_from_item(item) as im:\n",
    "            w, h = im.size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "        ratios.append(w / h)\n",
    "    return np.array(widths), np.array(heights), np.array(ratios)\n",
    "\n",
    "train_w, train_h, train_r = collect_sizes(train_ds)\n",
    "val_w, val_h, val_r = collect_sizes(val_ds)\n",
    "\n",
    "# Aggregate\n",
    "all_w = np.concatenate([train_w, val_w])\n",
    "all_h = np.concatenate([train_h, val_h])\n",
    "all_r = np.concatenate([train_r, val_r])\n",
    "\n",
    "# Print summary\n",
    "print(\"Counts — train/val/all:\", len(train_w), len(val_w), len(all_w))\n",
    "print(\"Width — min/median/max:\", int(all_w.min()), int(np.median(all_w)), int(all_w.max()))\n",
    "print(\"Height — min/median/max:\", int(all_h.min()), int(np.median(all_h)), int(all_h.max()))\n",
    "print(\"Aspect ratio (W/H) — min/median/max:\", round(all_r.min(), 4), round(float(np.median(all_r)), 4), round(all_r.max(), 4))\n",
    "\n",
    "# Most common resolutions\n",
    "res_pairs = list(zip(all_w.tolist(), all_h.tolist()))\n",
    "common_res = Counter(res_pairs).most_common(10)\n",
    "print(\"Top resolutions (W,H,count):\")\n",
    "for (w, h), c in common_res:\n",
    "    print((w, h, c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
